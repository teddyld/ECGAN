{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './data'\n",
    "RAFDB_PATH = DATA_PATH + '/RAFDB'\n",
    "MASKED_RAFDB_PATH = DATA_PATH + '/Masked_RAFDB'\n",
    "BINARY_RAFDB_PATH = DATA_PATH + '/Binary_RAFDB'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Inpainting Architecture \n",
    "\n",
    "Code for building blocks of generator and discriminators from https://github.com/daviddirethucus/Face-Mask_Inpainting\n",
    "Below is the original face inpainting architecture from 'A novel gan-based network for unmasking of masked face' by Nizam Ud Din et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(image,new_shape):\n",
    "    middle_height = image.shape[2] // 2\n",
    "    middle_width = image.shape[3] // 2\n",
    "    starting_height = middle_height - round(new_shape[2] / 2)\n",
    "    final_height = starting_height + new_shape[2]\n",
    "    starting_width = middle_width - round(new_shape[3] / 2)\n",
    "    final_width = starting_width+new_shape[3]\n",
    "    cropped_image = image[:, :, starting_height:final_height, starting_width:final_width]\n",
    "    return cropped_image\n",
    "\n",
    "class ContractingBlock(nn.Module):\n",
    "    def __init__(self, input_channels, use_in=True, use_dropout=False):\n",
    "        super(ContractingBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(input_channels, input_channels * 2, kernel_size=3, padding=1)\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        if use_in:\n",
    "            self.insnorm = nn.InstanceNorm2d(input_channels * 2)\n",
    "        self.use_in = use_in\n",
    "        if use_dropout:\n",
    "            self.drop = nn.Dropout()\n",
    "        self.use_dropout = use_dropout\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        if self.use_in:\n",
    "            x = self.insnorm(x)\n",
    "        if self.use_dropout:\n",
    "            x = self.drop(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.maxpool(x)\n",
    "        return x\n",
    "\n",
    "class ExpandingBlock(nn.Module):\n",
    "    def __init__(self,input_channels,use_in=True):\n",
    "        super(ExpandingBlock, self).__init__()\n",
    "        self.tconv = nn.ConvTranspose2d(input_channels, input_channels//2,kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.conv2 = nn.Conv2d(input_channels, input_channels//2, kernel_size=3, padding=1)\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "        if use_in:\n",
    "            self.insnorm = nn.InstanceNorm2d(input_channels//2)\n",
    "        self.use_in = use_in\n",
    "\n",
    "    def forward(self, x, skip_x):\n",
    "        x = self.tconv(x)\n",
    "        skip_x = crop(skip_x, x.shape)\n",
    "        x = torch.cat([x, skip_x], axis=1)\n",
    "        x = self.conv2(x)\n",
    "        if self.use_in:\n",
    "            x = self.insnorm(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class FeatureMapBlock(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(FeatureMapBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class SE_Block(nn.Module):\n",
    "    def __init__(self,channels,reduction=16):\n",
    "        super(SE_Block, self).__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        b, c, _, _ = x.shape\n",
    "        y = self.squeeze(x).view(b, c)\n",
    "        y = self.excitation(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "    \n",
    "class AtrousConv(nn.Module):\n",
    "    def __init__(self,input_channels):\n",
    "        super(AtrousConv, self).__init__()\n",
    "        self.aconv2 = nn.Conv2d(input_channels, input_channels, kernel_size=3, stride=1, dilation=2, padding=2)\n",
    "        self.aconv4 = nn.Conv2d(input_channels, input_channels, kernel_size=3, stride=1, dilation=4, padding=4)\n",
    "        self.aconv8 = nn.Conv2d(input_channels, input_channels, kernel_size=3, stride=1, dilation=8, padding=8)\n",
    "        self.aconv16 = nn.Conv2d(input_channels, input_channels, kernel_size=3, stride=1, dilation=16, padding=16)\n",
    "        self.batchnorm = nn.BatchNorm2d(input_channels)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.aconv2(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = self.aconv4(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = self.aconv8(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = self.aconv16(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECGAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetIICGAN(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, hidden_channels=32, num_classes=7):\n",
    "        super(UNetIICGAN, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.label_emb = nn.Embedding(num_classes, input_channels + 1)\n",
    "\n",
    "        self.upfeature = FeatureMapBlock(input_channels + num_classes, hidden_channels)\n",
    "        self.contract1 = ContractingBlock(hidden_channels, use_in=False, use_dropout=True)\n",
    "        self.contract2 = ContractingBlock(hidden_channels * 2, use_dropout=True)\n",
    "        self.contract3 = ContractingBlock(hidden_channels * 4, use_dropout=True)\n",
    "        self.contract4 = ContractingBlock(hidden_channels * 8)\n",
    "        self.contract5 = ContractingBlock(hidden_channels * 16)\n",
    "\n",
    "        self.atrous_conv = AtrousConv(hidden_channels * 32)\n",
    "\n",
    "        self.expand0 = ExpandingBlock(hidden_channels * 32)\n",
    "        self.expand1 = ExpandingBlock(hidden_channels * 16)\n",
    "        self.expand2 = ExpandingBlock(hidden_channels * 8)\n",
    "        self.expand3 = ExpandingBlock(hidden_channels * 4)\n",
    "        self.expand4 = ExpandingBlock(hidden_channels * 2)\n",
    "        self.downfeature = FeatureMapBlock(hidden_channels, output_channels)\n",
    "\n",
    "        self.se1 = SE_Block(hidden_channels * 2)\n",
    "        self.se2 = SE_Block(hidden_channels * 4)\n",
    "        self.se3 = SE_Block(hidden_channels * 8)\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        # convert labels to embeddings\n",
    "        label_emb = self.label_emb(labels).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        # concatenate the label embeddings with the input tensor\n",
    "        x = torch.cat((x, label_emb.repeat(1, 1, x.size(2), x.size(3))), dim=1)\n",
    "\n",
    "        x0 = self.upfeature(x)\n",
    "        x1 = self.contract1(x0)\n",
    "        x1 = self.se1(x1)\n",
    "        x2 = self.contract2(x1)\n",
    "        x2 = self.se2(x2)\n",
    "        x3 = self.contract3(x2)\n",
    "        x3 = self.se3(x3)\n",
    "        x4 = self.contract4(x3)\n",
    "        x5 = self.contract5(x4)\n",
    "        x5 = self.atrous_conv(x5)\n",
    "        x6 = self.expand0(x5, x4)\n",
    "        x7 = self.expand1(x6, x3)\n",
    "        x8 = self.expand2(x7, x2)\n",
    "        x9 = self.expand3(x8, x1)\n",
    "        x10 = self.expand4(x9, x0)\n",
    "        xn = self.downfeature(x10)\n",
    "\n",
    "        return self.tanh(xn)\n",
    "\n",
    "class Discriminator_whole_CGAN(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels=8):\n",
    "        super(Discriminator_whole_CGAN, self).__init__()\n",
    "        self.label_emb = nn.Embedding(7, 3)\n",
    "        self.upfeature = FeatureMapBlock(12, hidden_channels)\n",
    "        self.contract1 = ContractingBlock(hidden_channels, use_in=False)\n",
    "        self.contract2 = ContractingBlock(hidden_channels * 2)\n",
    "        self.contract3 = ContractingBlock(hidden_channels * 4)\n",
    "        self.contract4 = ContractingBlock(hidden_channels * 8)\n",
    "        self.final = nn.Conv2d(hidden_channels*16, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, y, labels):\n",
    "        # gt, input_imgs, labels\n",
    "        # compute the label embedding\n",
    "        label_emb = self.label_emb(labels)\n",
    "        label_emb = label_emb.unsqueeze(2).unsqueeze(3).repeat(1, 1, x.size(2), x.size(3))\n",
    "\n",
    "        # concatenate the label embedding and input tensors\n",
    "        x = torch.cat([x, y, label_emb], dim=1)\n",
    "\n",
    "        # pass the tensor through the network\n",
    "        x0 = self.upfeature(x)\n",
    "        x1 = self.contract1(x0)\n",
    "        x2 = self.contract2(x1)\n",
    "        x3 = self.contract3(x2)\n",
    "        x4 = self.contract4(x3)\n",
    "        xn = self.final(x4)\n",
    "        return xn\n",
    "\n",
    "class Discriminator_mask_CGAN(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels=8):\n",
    "        super(Discriminator_mask_CGAN, self).__init__()\n",
    "        self.label_emb =nn.Embedding(7, 3)\n",
    "        self.upfeature = FeatureMapBlock(12, hidden_channels)\n",
    "        self.contract1 = ContractingBlock(hidden_channels, use_in=False)\n",
    "        self.contract2 = ContractingBlock(hidden_channels * 2)\n",
    "        self.contract3 = ContractingBlock(hidden_channels * 4)\n",
    "        self.contract4 = ContractingBlock(hidden_channels * 8)\n",
    "        self.final = nn.Conv2d(hidden_channels*16, 1, kernel_size=1)\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, x, y, labels):\n",
    "        # compute the label embedding\n",
    "        label_emb = self.label_emb(labels)\n",
    "        label_emb = label_emb.unsqueeze(2).unsqueeze(3).repeat(1, 1, x.size(2), x.size(3))\n",
    "\n",
    "        # concatenate the label embedding and input tensors\n",
    "        x = torch.cat([x, y, label_emb], dim=1)\n",
    "\n",
    "        # pass the tensor through the network\n",
    "        x0 = self.upfeature(x)\n",
    "        x1 = self.contract1(x0)\n",
    "        x2 = self.contract2(x1)\n",
    "        x2 = self.dropout(x2)\n",
    "        x3 = self.contract3(x2)\n",
    "        x4 = self.contract4(x3)\n",
    "        xn = self.final(x4)\n",
    "        return xn\n",
    "\n",
    "def loadm(model, state): # load model pretrained\n",
    "   model_state_dict = model.state_dict()\n",
    "   for key in state:\n",
    "      if  ((key == 'upfeature.conv.weight')) :\n",
    "        pass\n",
    "      else:\n",
    "        model_state_dict[key] = state[key]\n",
    "\n",
    "   model.load_state_dict(model_state_dict, strict = False)\n",
    "   return model\n",
    "\n",
    "def cgan_inpaint_in(model_path):\n",
    "    lr = 0.0003\n",
    "    input_dim = 6\n",
    "    output_dim = 3\n",
    "    disc_dim = 9\n",
    "\n",
    "    gen = UNetIICGAN(input_dim, output_dim).to(device)\n",
    "    gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n",
    "    disc_whole = Discriminator_whole_CGAN(disc_dim).to(device)\n",
    "    disc_whole_opt = torch.optim.Adam(disc_whole.parameters(), lr=0.0001)\n",
    "    disc_mask = Discriminator_mask_CGAN(disc_dim).to(device)\n",
    "    disc_mask_opt = torch.optim.Adam(disc_mask.parameters(), lr=0.0001)\n",
    "\n",
    "    loaded_state = torch.load(model_path, map_location=torch.device(device))\n",
    "    gen = loadm(gen, loaded_state[\"gen\"])\n",
    "    disc_whole = loadm(disc_whole, loaded_state[\"disc_whole\"])\n",
    "    disc_mask = loadm(disc_mask, loaded_state[\"disc_mask\"])\n",
    "\n",
    "    return gen, gen_opt, disc_whole, disc_whole_opt, disc_mask, disc_mask_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAFDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        unmask_path,\n",
    "        mask_path,\n",
    "        binary_path,\n",
    "        split,\n",
    "        transform=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            unmask_path (string): path to non-masked directory\n",
    "            mask_path (string): path to masked directory\n",
    "            binary_path (string): path to binary mask directory\n",
    "            split (string): the target split i.e. 'train' or 'test'\n",
    "            transform (optional, callable): optional transform to apply on image\n",
    "        \"\"\"\n",
    "        labels_df = pd.read_csv(os.path.join(unmask_path, f\"{split}_labels.csv\"))\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "        # Binary mask transform\n",
    "        self.binary_tf = transforms.Compose([\n",
    "            transforms.Resize(size=(48, 48)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        unmask_path = os.path.join(unmask_path, split)\n",
    "        mask_path = os.path.join(mask_path, split)\n",
    "        binary_path = os.path.join(binary_path, split)\n",
    "        \n",
    "        unmask_all_files = os.listdir(unmask_path)\n",
    "        mask_all_files = os.listdir(mask_path)\n",
    "        binary_all_files = os.listdir(binary_path)\n",
    "        \n",
    "        \n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        for i in range(0, len(unmask_all_files)):\n",
    "            item = (\n",
    "                os.path.join(unmask_path, unmask_all_files[i]),\n",
    "                os.path.join(mask_path, mask_all_files[i]),\n",
    "                os.path.join(binary_path, binary_all_files[i]),\n",
    "            )\n",
    "            self.data.append(item)\n",
    "            label = labels_df.loc[labels_df['image'] == unmask_all_files[i]]['label'].item()\n",
    "            self.labels.append(label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        unmasked_img_path, masked_img_path, binary_img_path = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        unmasked_img = Image.open(unmasked_img_path)\n",
    "        masked_img = Image.open(masked_img_path)\n",
    "        binary_img = Image.open(binary_img_path).convert('RGB')\n",
    "        \n",
    "        binary_img = self.binary_tf(binary_img)\n",
    "\n",
    "        if self.transform:\n",
    "            unmasked_img = self.transform(unmasked_img)\n",
    "            masked_img = self.transform(masked_img)\n",
    "        \n",
    "        return unmasked_img, masked_img, binary_img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Augmentation and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size=(48, 48)), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RAFDataset(RAFDB_PATH, MASKED_RAFDB_PATH, BINARY_RAFDB_PATH, 'train', transform)\n",
    "valid_dataset = RAFDataset(RAFDB_PATH, MASKED_RAFDB_PATH, BINARY_RAFDB_PATH, 'test', transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions\n",
    "\n",
    "To train ECGAN for face inpainting we combine discriminator, adversarial, and reconstruction losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from math import exp\n",
    "\n",
    "def normalize(img):\n",
    "    return (img - (-1)) / (1 - (-1))\n",
    "def anti_normalize(img):\n",
    "    return img * (1 - (-1)) + (-1)\n",
    "\n",
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()\n",
    "\n",
    "def create_window(window_size, channel):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
    "    return window\n",
    "\n",
    "def _ssim(img1, img2, window, window_size, channel, size_average = True):\n",
    "    mu1 = F.conv2d(img1, window, padding=window_size//2, groups=channel)\n",
    "    mu2 = F.conv2d(img2, window, padding=window_size//2, groups=channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1*mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2\n",
    "\n",
    "    C1 = 0.01**2\n",
    "    C2 = 0.03**2\n",
    "\n",
    "    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "    if size_average:\n",
    "        return ssim_map.mean()\n",
    "    else:\n",
    "        return ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "class SSIM(torch.nn.Module):\n",
    "    def __init__(self, window_size = 11, size_average = True):\n",
    "        super(SSIM, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.size_average = size_average\n",
    "        self.channel = 1\n",
    "        self.window = create_window(window_size, self.channel)\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        ###\n",
    "        img1 = (img1+1)/2\n",
    "        img2 = (img2+1)/2\n",
    "        ###\n",
    "        (_, channel, _, _) = img1.size()\n",
    "\n",
    "        if channel == self.channel and self.window.data.type() == img1.data.type():\n",
    "            window = self.window\n",
    "        else:\n",
    "            window = create_window(self.window_size, channel)\n",
    "\n",
    "            if img1.is_cuda:\n",
    "                window = window.cuda(img1.get_device())\n",
    "            window = window.type_as(img1)\n",
    "\n",
    "            self.window = window\n",
    "            self.channel = channel\n",
    "\n",
    "\n",
    "        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n",
    "def recon_loss(gt,fake,recon_criterion):\n",
    "    ssim = SSIM()\n",
    "    ssim_loss = ssim(gt,fake)\n",
    "    l1_loss = recon_criterion(gt,fake)\n",
    "    return l1_loss,ssim_loss\n",
    "\n",
    "# Perceptual Loss\n",
    "\n",
    "from torchvision.models import vgg19\n",
    "\n",
    "class PerceptualNet(nn.Module):\n",
    "    def __init__(self, name = \"vgg19\", resize=True):\n",
    "        super(PerceptualNet, self).__init__()\n",
    "        blocks = []\n",
    "        blocks.append(vgg19(pretrained=True).features[:4].eval())\n",
    "        blocks.append(vgg19(pretrained=True).features[4:9].eval())\n",
    "        blocks.append(vgg19(pretrained=True).features[9:16].eval())\n",
    "        blocks.append(vgg19(pretrained=True).features[16:23].eval())\n",
    "\n",
    "        for bl in blocks:\n",
    "            for p in bl:\n",
    "                p.requires_grad = False\n",
    "        self.blocks = torch.nn.ModuleList(blocks).to(device)\n",
    "        self.transform = torch.nn.functional.interpolate\n",
    "        self.mean = torch.nn.Parameter(torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1)).to(device)\n",
    "        self.std = torch.nn.Parameter(torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1)).to(device)\n",
    "        self.resize = resize\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if inputs.shape[1] != 3:\n",
    "            inputs = inputs.repeat(1, 3, 1, 1)\n",
    "            targets = targets.repeat(1, 3, 1, 1)\n",
    "        inputs = (inputs+1)/2\n",
    "        targets = (targets+1)/2\n",
    "        if self.resize:\n",
    "            inputs = self.transform(inputs, mode='bilinear', size=(224, 224), align_corners=False)\n",
    "            targets = self.transform(targets, mode='bilinear', size=(224, 224), align_corners=False)\n",
    "        loss = 0.0\n",
    "        x = inputs\n",
    "        y = targets\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            y = block(y)\n",
    "            loss += torch.nn.functional.l1_loss(x, y)\n",
    "        return loss\n",
    "\n",
    "def percep_loss(gt,fake):\n",
    "    percep_net = PerceptualNet()\n",
    "    return percep_net(gt, fake)\n",
    "\n",
    "def discwhole_loss_func(disc_whole, gt, mask, binary, label, fake, adv_criterion, lambda_Dwhole):\n",
    "    input_imgs = torch.cat((mask, binary),1)\n",
    "    fake_pred = disc_whole(fake.detach(),input_imgs, label)\n",
    "    gt_pred = disc_whole(gt,input_imgs, label)\n",
    "    fake_loss = adv_criterion(fake_pred,torch.zeros_like(fake_pred))\n",
    "    gt_loss = adv_criterion(gt_pred,torch.ones_like(gt_pred))\n",
    "    return lambda_Dwhole * (fake_loss+gt_loss)/2\n",
    "\n",
    "\n",
    "def discmask_loss_func(disc_mask, gt, fake, mask, binary, label, adv_criterion, lambda_Dmask):\n",
    "    nor_mask = normalize(mask)\n",
    "    nor_binary = normalize(binary)\n",
    "    nor_fake = normalize(fake)\n",
    "\n",
    "    oofs = torch.mul(nor_mask,1-nor_binary)\n",
    "    oops = torch.mul(nor_fake,nor_binary)\n",
    "    ooo = anti_normalize(oofs+oops)\n",
    "    input_imgs = torch.cat((mask,binary),1)\n",
    "    fake_pred = disc_mask(ooo.detach(),input_imgs, label)\n",
    "    gt_pred = disc_mask(gt,input_imgs, label)\n",
    "\n",
    "    fake_loss = adv_criterion(fake_pred,torch.zeros_like(fake_pred))\n",
    "    gt_loss = adv_criterion(gt_pred,torch.ones_like(gt_pred))\n",
    "\n",
    "    return lambda_Dmask * (fake_loss+gt_loss)/2\n",
    "\n",
    "\n",
    "def gen_adv_loss(gen,disc, gt, mask,binary, label, adv_criterion):\n",
    "    input_imgs = torch.cat((mask,binary),1)\n",
    "    fake = gen(input_imgs, label)\n",
    "    fake_pred = disc(fake,input_imgs, label)\n",
    "    adv_loss = adv_criterion(fake_pred,torch.ones_like(fake_pred))\n",
    "    return adv_loss,fake\n",
    "\n",
    "def generator_loss(cur_step, gen, disc_whole, disc_mask, gt, mask, binary, label, adv_criterion, recon_criterion, lambda_recon, lambda_adv_whole, lambda_adv_mask):\n",
    "    if cur_step < 3516 * 6:\n",
    "        adver_loss_whole,fake = gen_adv_loss(gen,disc_whole,gt,mask,binary,label,adv_criterion)\n",
    "        l1_loss,ssim_loss = recon_loss(gt,fake,recon_criterion)\n",
    "        reconstruction_loss = l1_loss * 0.5 + (1 - ssim_loss) * 0.5\n",
    "        perceptual_loss = percep_loss(gt, fake)\n",
    "        gen_loss = lambda_recon * (reconstruction_loss + perceptual_loss)+lambda_adv_whole*adver_loss_whole\n",
    "    else:\n",
    "        adver_loss_whole,fake = gen_adv_loss(gen, disc_whole, gt, mask, binary,label, adv_criterion)\n",
    "        adver_loss_mask,fake = gen_adv_loss(gen, disc_mask, gt, mask, binary,label, adv_criterion)\n",
    "        l1_loss,ssim_loss = recon_loss(gt, fake,recon_criterion)\n",
    "        reconstruction_loss = l1_loss*0.5 + (1-ssim_loss)*0.5\n",
    "        perceptual_loss = percep_loss(gt,fake)\n",
    "        gen_loss = lambda_recon*(reconstruction_loss+perceptual_loss)+lambda_adv_whole*adver_loss_whole+lambda_adv_mask*adver_loss_mask\n",
    "\n",
    "\n",
    "    return gen_loss,fake,l1_loss,ssim_loss,perceptual_loss\n",
    "\n",
    "# FID\n",
    "\n",
    "from torchvision.models import inception_v3\n",
    "import scipy.linalg\n",
    "\n",
    "inception_model = inception_v3(pretrained=True)\n",
    "inception_model.to(device)\n",
    "inception_model = inception_model.eval() # Evaluation mode\n",
    "inception_model.fc = torch.nn.Identity()\n",
    "\n",
    "def matrix_sqrt(x):\n",
    "    y = x.cpu().detach().numpy()\n",
    "    y = scipy.linalg.sqrtm(y)\n",
    "    return torch.Tensor(y.real,device=x.device)\n",
    "\n",
    "def frechet_distance(mu_x,mu_y,sigma_x,sigma_y):\n",
    "    return torch.norm(mu_x-mu_y)**2 + torch.trace(sigma_x+sigma_y-2*matrix_sqrt(sigma_x@sigma_y))\n",
    "\n",
    "def get_covariance(features):\n",
    "    return torch.Tensor(np.cov(features.detach().numpy(),rowvar=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_criterion = nn.BCEWithLogitsLoss()\n",
    "recon_criterion = nn.L1Loss()\n",
    "lambda_recon = 100\n",
    "lambda_Dwhole = 0.3\n",
    "lambda_Dmask = 0.7\n",
    "lambda_adv_whole = 0.3\n",
    "lambda_adv_mask = 0.7\n",
    "\n",
    "num_epochs = 2\n",
    "input_dim = 6\n",
    "output_dim = 3\n",
    "disc_dim = 9\n",
    "lr = 0.0003\n",
    "\n",
    "model_path = './models/Inpaint_UNet.pth'\n",
    "gen, gen_opt, disc_whole, disc_whole_opt, disc_mask, disc_mask_opt = cgan_inpaint_in(model_path)\n",
    "gen.to(device)\n",
    "disc_whole.to(device)\n",
    "disc_mask.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPRESSION_MAP = {\n",
    "    1: 'Surprise',\n",
    "    2: 'Fear',\n",
    "    3: 'Disgust',\n",
    "    4: 'Happy',\n",
    "    5: 'Sad',\n",
    "    6: 'Angry',\n",
    "    7: 'Neutral'\n",
    "}\n",
    "\n",
    "def img_to_display(images, display_title, labels=None, num_samples=3):\n",
    "    \"\"\"Plot num_sample images\n",
    "    \n",
    "    Args:\n",
    "        images (tensor -> shape(B, C, H, W)): Defines the tensor of images to display\n",
    "        display_title (string): Defines the title of the figure\n",
    "        labels (tensor -> shape[B]): The ground truth expression labels for the unmasked images\n",
    "        num_samples (int): Defines the number of images to sample and display\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if num_samples > images.shape[0]:\n",
    "        assert ValueError(\"num_samples was greater than the image batch_size\")\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=num_samples, figsize=(10, 5), subplot_kw={'xticks': [], 'yticks': []})\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        image = images[i].detach().cpu().permute(1, 2, 0)\n",
    "        image = (image + 1) / 2\n",
    "        ax.imshow(image)\n",
    "        \n",
    "        # Write label as ax title\n",
    "        if labels != None:\n",
    "            label = labels[i].detach().cpu().item()\n",
    "            ax.set_title(EXPRESSION_MAP[label])\n",
    "    \n",
    "    fig.suptitle(display_title, y=0.87, size=18)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def ecgan_train(gen, disc_whole, disc_mask, disc_whole_opt, disc_mask_opt, train_loader, valid_loader, num_epochs, cur_step=0, display_step=400, save_model=True):\n",
    "    gen.train()\n",
    "    disc_whole.train()\n",
    "    disc_mask.train()\n",
    "\n",
    "    mean_generator_loss = 0\n",
    "    mean_disc_whole_loss = 0\n",
    "    mean_disc_mask_loss = 0\n",
    "    fake_features_list = []\n",
    "    real_features_list = []\n",
    "    \n",
    "    for epoch in range(0, num_epochs):\n",
    "        train_loop = tqdm(train_loader, leave=False)\n",
    "        for gt, mask, binary, labels in train_loop:\n",
    "            gt = gt.to(device)\n",
    "            mask = mask.to(device)\n",
    "            binary = binary.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                input_imgs = torch.cat((mask, binary), 1)\n",
    "                # fake = gen(input_imgs, labels)\n",
    "            \n",
    "            # Display unmasked, masked, and binary images at each display_step\n",
    "            if cur_step % display_step == 0:\n",
    "                img_to_display(gt, 'Ground truth', labels)\n",
    "                img_to_display(mask, 'Masked images')\n",
    "                # img_to_display(fake, 'Fake images')\n",
    "            \n",
    "            \n",
    "            cur_step += 1\n",
    "            train_loop.set_description(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecgan_train(\n",
    "    gen,\n",
    "    disc_whole,\n",
    "    disc_mask,\n",
    "    disc_whole_opt,\n",
    "    disc_mask_opt,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    cur_step=0,\n",
    "    display_step=400,\n",
    "    save_model=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
